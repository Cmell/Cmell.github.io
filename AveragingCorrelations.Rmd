---
title: "Correlation Averages"
author: "Chris Mellinger"
date: "8/8/2018"
output: html_document
---

Reproducible simulations require a seed.

```{r}
set.seed(30482)
```


# Example 1: Same slope, same errors, different intercept

This example shows that the average of two correlation coefficients calculated from different sets of data do not equal the correlation observed if the two sets of data are treated as a single set. I constructed two models with almost exactly the same correlation coefficient by simulating data with very similar error terms (and x values for simplicity). The average of those two equal numbers is, obviously, close to equal to the value of either. However, the intercepts that each model was generated with are very different. As a result, the correlation between the variables comprising the full dataset is much lower.

```{r}
n <- 200

x1 <- seq(0, 10, length.out = n/2)
x2 <- seq(0, 10, length.out = n/2)

errs <- rnorm(n/2, sd = 1)

y1 <- 10 + 1 * x1 + rnorm(n/2)
y2 <- -10 + 1 * x2 + rnorm(n/2)

xAll <- c(x1, x2); yAll <- c(y1, y2)

print(c1 <- cor(x1, y1))
print(c2 <- cor(x2, y2))
print(avgc <- mean(c(c1, c2)))
print(cor(xAll, yAll))
```

Why? Because a correlation coefficient captures an effect size, not just a slope in the original metric of the data. The sums of squares for the best-fit line of the whole data set are obviously much larger than the sums of squares for either subset of the data. It is easy to see when all of them are plotted. The predictions for each subset (blue and red lines) are much closer to the points than the prediction for the whole set (green line).

```{r}
plot(xAll, yAll, type = 'n')
points(x1, y1, col = 'red'); points(x2, y2, col = 'blue')
abline(lm(y1 ~ x1), col = 'red')
abline(lm(y2 ~ x2), col = 'blue')

summary(lm(y1 ~ x1))
summary(lm(y2 ~ x2))

avgMod <- lm(yAll ~ xAll)
abline(avgMod, col = 'forestgreen')
```

Of course, if the variables are standardized and plotted, things look different. The red crosses and blue points (each subset) are now close to one another since I generated them using the similar error sets. The standardized full set is now split in two, and the best-fit line has a lower slope than the subsets. This lower slope is reflected by the lower correlation coefficient.

```{r}
xZ1 <- scale(x1); xZ2 <- scale(x2); yZ1 <- scale(y1); yZ2 <- scale(y2)
xZAll <- scale(xAll); yZAll <- scale(yAll)

plot(c(xZ1, xZ2, xZAll), c(yZ1, yZ2, yZAll), type = 'n')
points(xZ1, yZ1, col = 'red'); points(xZ2, yZ2, col = 'blue')
points(xZAll, yZAll, col = 'forestgreen')
abline(lm(yZ1 ~ xZ1), col = 'red')
abline(lm(yZ2 ~ xZ2), col = 'blue')
abline(lm(yZAll ~ xZAll), col = 'forestgreen')
```

# Example 2: Different slopes, same errors, same intercept

What is the effect of chaning the slopes of the underlying models instead of the intercepts?

```{r}
y1 <- .9 * x1 + rnorm(n/2)
y2 <- .2 * x2 + rnorm(n/2)

xAll <- c(x1, x2); yAll <- c(y1, y2)

print(c1 <- cor(x1, y1))
print(c2 <- cor(x2, y2))
print(avc <- mean(c(c1, c2)))
print(cor(xAll, yAll))
```

Again, the average isn't a super good estimate. Since the points are generally closer to each other in this range, things look better but not great.

```{r}
plot(xAll, yAll, type = 'n')
points(x1, y1, col = 'red'); points(x2, y2, col = 'blue')
abline(lm(y1 ~ x1), col = 'red')
abline(lm(y2 ~ x2), col = 'blue')

summary(lm(y1 ~ x1))
summary(lm(y2 ~ x2))

avgMod <- lm(yAll ~ xAll)
abline(avgMod, col = 'forestgreen')
```

# Example 3: Randomly sample a subset of a population

The previous two examples seem like relatively rare situations. Generating two different data setsaround two different lines basically dooms this strategy from the outset. What about a more realistic situation where we randomly sample from a population with a single underlying correlation?

```{r}
xAll <- runif(n)
yAll <- .6 * xAll + rnorm(n)

cor(xAll, yAll)
mod <- lm(yAll ~ xAll)
plot(xAll, yAll)
abline(mod)
```

Now, choose two non-overlapping subsets of equal size.

```{r}
nSmall <- n / 2
sampleIndices <- sample(1:length(xAll), nSmall * 2)
set1Indices <- sampleIndices[1:(length(sampleIndices) / 2)]
set2Indices <- sampleIndices[(length(sampleIndices) / 2 + 1):length(sampleIndices)]

x1 <- xAll[set1Indices]; x2 <- xAll[set2Indices]
y1 <- yAll[set1Indices]; y2 <- yAll[set2Indices]

print(c1 <- cor(x1, y1))
print(c2 <- cor(x2, y2))
print(mean(c(c1, c2)))
print(cor(xAll, yAll))
```

Now we are a lot closer! Let's plot.

```{r}
plot(xAll, yAll, type = 'n')
points(x1, y1, col = 'red'); points(x2, y2, col = 'blue')
abline(lm(y1 ~ x1), col = 'red')
abline(lm(y2 ~ x2), col = 'blue')

summary(lm(y1 ~ x1))
summary(lm(y2 ~ x2))

avgMod <- lm(yAll ~ xAll)
abline(avgMod, col = 'forestgreen')
```

## Example 3 addendum: Different sized subsets

```{r}
nSmall1 <- floor(n / 5)
nSmall2 <- n - nSmall
sampleIndices <- sample(1:length(xAll), n)
set1Indices <- sampleIndices[1:nSmall1]
set2Indices <- sampleIndices[nSmall2:length(sampleIndices)]

x1 <- xAll[set1Indices]; x2 <- xAll[set2Indices]
y1 <- yAll[set1Indices]; y2 <- yAll[set2Indices]

print(c1 <- cor(x1, y1))
print(c2 <- cor(x2, y2))
print(mean(c(c1, c2)))
print(cor(xAll, yAll))
```

That messes things up a bit. Now the true correlation coefficient is estimated better by the bigger set, so averaging the two subsets is not good.

```{r}
plot(xAll, yAll, type = 'n')
points(x1, y1, col = 'red'); points(x2, y2, col = 'blue')
abline(lm(y1 ~ x1), col = 'red')
abline(lm(y2 ~ x2), col = 'blue')

summary(lm(y1 ~ x1))
summary(lm(y2 ~ x2))

avgMod <- lm(yAll ~ xAll)
abline(avgMod, col = 'forestgreen')
```

# Conclusion

In general, correlation coefficients cannot be averaged. The answer will be close if the two sets are randomly selected from a population with a common correlation and the sets are similar sizes. However, if the subsets are systematically different in any way in the underlying model, or if they are different sizes, the averaged correlation will be a very biased estimate. In practice, this is probably not a good idea.
